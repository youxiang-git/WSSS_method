{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/youxiang/anaconda3/envs/voc_wsss/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/youxiang/anaconda3/envs/voc_wsss/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392026823/work/aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torchvision.ops import box_convert\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = load_model(\"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"GroundingDINO/weights/groundingdino_swint_ogc.pth\")\n",
    "# IMAGE_PATH = \"/home/youxiang/Desktop/FYP/WSSS_method/VOCtrainval_11-May-2012/VOCdevkit/VOC2012/JPEGImages/2007_000032.jpg\"\n",
    "# TEXT_PROMPT = \"aeroplane. person.\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "curr_dir = os.getcwd()\n",
    "voc_root = os.path.join(curr_dir, \"VOCtrainval_11-May-2012\", \"VOCdevkit\", \"VOC2012\")\n",
    "\n",
    "if not os.path.isdir(voc_root):\n",
    "    raise RuntimeError(\"VOC Directory is wrong\")\n",
    "\n",
    "splits_dir = os.path.join(voc_root, \"ImageSets\", \"Segmentation\")\n",
    "splits_f = os.path.join(splits_dir, \"{}.txt\".format(\"train\"))\n",
    "\n",
    "with open(os.path.join(splits_f)) as f:\n",
    "    file_names = [x.strip() for x in f.readlines()]\n",
    "\n",
    "img_dir = os.path.join(voc_root, \"JPEGImages\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/youxiang/anaconda3/envs/voc_wsss/lib/python3.11/site-packages/transformers/modeling_utils.py:905: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/youxiang/anaconda3/envs/voc_wsss/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/youxiang/anaconda3/envs/voc_wsss/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for x in file_names:\n",
    "  point_file = \"voc_points/{}_points.json\".format(x)\n",
    "  image_loc = os.path.join(img_dir, \"{}.jpg\".format(x))\n",
    "  image_source, image = load_image(image_loc)\n",
    "\n",
    "  with open(point_file, \"r\") as input_file:\n",
    "    whole_input = json.load(input_file)\n",
    "\n",
    "  if whole_input == [] or len(whole_input) == 0:\n",
    "    print(\"input is empty\")\n",
    "    continue\n",
    "  \n",
    "  categories = []\n",
    "  # points = []\n",
    "  for object in whole_input:\n",
    "    category = object[\"category\"]\n",
    "    # sampled_points = object[\"sampled_points\"]\n",
    "    categories.append(category)\n",
    "    # points.append(sampled_points)\n",
    "\n",
    "  categories = list(set(categories))\n",
    "  cat_str = '. '.join(categories) + \".\"\n",
    "\n",
    "  boxes, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=cat_str,\n",
    "    box_threshold=BOX_TRESHOLD,\n",
    "    text_threshold=TEXT_TRESHOLD\n",
    "  )\n",
    "\n",
    "  h, w, _ = image_source.shape\n",
    "  boxes = boxes * torch.tensor([w, h, w, h])\n",
    "  xyxy = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "  xyxy = torch.tensor(xyxy, dtype=torch.int32)\n",
    "  xyxy = xyxy.numpy().astype(int)\n",
    "  # sort by area\n",
    "  areas = []\n",
    "  for box in xyxy:\n",
    "    x1, y1, x2, y2 = box\n",
    "    area = (x2 - x1) * (y2 - y1)\n",
    "    areas.append(area)\n",
    "\n",
    "  k = len(areas)\n",
    "  # print(areas.index(max(areas)))\n",
    "  sorted_boxes = []\n",
    "  for idx in range(k):\n",
    "    j = areas.index(min(areas))\n",
    "    sorted_boxes.append(xyxy[j])\n",
    "    areas[j] = float('inf')\n",
    "\n",
    "  # print(sorted_boxes)\n",
    "  #It's in the form of Top-left, bottom-right\n",
    "  # print(xyxy)\n",
    "  for object2 in whole_input:\n",
    "    # print(object2['category'])\n",
    "    points = object2[\"sampled_points\"]\n",
    "    for i in range(len(sorted_boxes)):\n",
    "      flag = 0  \n",
    "      box = sorted_boxes[i]\n",
    "      # print(box)\n",
    "      x1, y1, x2, y2 = box\n",
    "      assert x2 > x1 and y2 > y1\n",
    "      for point in points:\n",
    "        x, y = point\n",
    "        if x >= x1 and x <= x2 and y >= y1 and y <= y2:\n",
    "          flag+= 1\n",
    "      if flag >= 2:\n",
    "        box = box.tolist()\n",
    "        # print(box)\n",
    "        object2.update({\"box\": box})\n",
    "        break\n",
    "\n",
    "  with open(point_file, \"w\") as output_file:\n",
    "    json.dump(whole_input, output_file)\n",
    "\n",
    "\n",
    "\n",
    "# boxes, logits, phrases = predict(\n",
    "#     model=model,\n",
    "#     image=image,\n",
    "#     caption=TEXT_PROMPT,\n",
    "#     box_threshold=BOX_TRESHOLD,\n",
    "#     text_threshold=TEXT_TRESHOLD,\n",
    "# )\n",
    "\n",
    "# annotated_frame = annotate(\n",
    "#     image_source=image_source, boxes=boxes, logits=logits, phrases=phrases\n",
    "# )\n",
    "# plt.imshow(\"annotated_image.jpg\", annotated_frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voc_wsss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
